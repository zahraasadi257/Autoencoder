{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yixXIZ-zeXTi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.ToTensor()  #  Convert images to pytorch Tensor\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize((0.5),(0.5))\n",
        "#  ])\n",
        "mnist_data = datasets.MNIST(root = \"./data\", train = True, download = True, transform=transform) # download the data and save it in ./data\n",
        "data_loader = torch.utils.data.DataLoader(dataset = mnist_data, batch_size = 64, shuffle = True)"
      ],
      "metadata": {
        "id": "TJ4cxC03e2dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataiter = iter(data_loader)    # To see how the data looks like; inspect the first image or the first batch\n",
        "# images,labels = dataiter.next()\n",
        "# print(torch.min(images), torch.max(images)) # This mayy change, if we chane the transform type"
      ],
      "metadata": {
        "id": "k_1SYpNBidsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder_Linear(nn.Module):\n",
        "  def __init__(self):\n",
        "    #N: number of batches, 784 = 28x28, we want to reduce the size\n",
        "    # A good way to structure your model in an autoencoder is to use a sequential model\n",
        "    super().__init__()\n",
        "    self.encoder = nn.Sequential(\n",
        "        nn.Linear(28*28, 128), # Reduce N, 784 to N,128\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128,64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64,12),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(12,3)   #N,3\n",
        "        # In the last layer, we dont need an activation function\n",
        "    )\n",
        "    self.decoder = nn.Sequential(\n",
        "        # Increase N, 3 to N,784\n",
        "        nn.Linear(3, 12),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(12,64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64,128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128,28*28),\n",
        "        nn.Sigmoid()  # in spite of encoder, we need an activation function to put the values of images in torch.min(images), torch.max(images)\n",
        "\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "\n",
        "# Note: Keep the last layer in mind: If our input images be in [-1,1], then, we dont need the sigmoid and instead apply tanh, this can happen if we apply a normalization transform"
      ],
      "metadata": {
        "id": "cJ-e6aZDicfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder_CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    #N, 1, 28, 28\n",
        "    # A good way to structure your model in an autoencoder is to use a sequential model\n",
        "    super().__init__()\n",
        "    self.encoder = nn.Sequential(\n",
        "        nn.Conv2d(1, 16, 3, stride = 2, padding=1), # N, 16 channels, 14, 14 size\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(16 , 32 ,3, stride = 2, padding=1), # N, 32 channels, 7, 7 size\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 64, 7)  # N, 64 channels, 1, 1 size\n",
        "\n",
        "        # In the last layer, we dont need an activation function\n",
        "    )\n",
        "    # N, 64 channels, 1, 1 size\n",
        "    self.decoder = nn.Sequential(\n",
        "\n",
        "        nn.ConvTranspose2d(64, 32, 7), #N, 32, 7,7\n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(32 ,16, 3, stride = 2, padding=1, output_padding= 1 ), #N, 16, 13, 13: output_padding = 1 put zeros in the margine to convert it to 14, 14\n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(16, 1, 3,  stride = 2, padding=1, output_padding= 1 ),  #N, 1, 27, 27: output_padding = 1 put zeros in the margin to convert it to 28, 28\n",
        "        nn.Sigmoid()  # in spite of encoder, we need an activation function to put the values of images in torch.min(images), torch.max(images)\n",
        "\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "\n",
        "# Note: Keep the last layer in mind: If our input images be in [-1,1], then, we dont need the sigmoid and instead apply tanh, this can happen if we apply a normalization transform\n",
        "\n",
        "# nn.MaxPool2d  reduces the size VS nn.MaxUnpool2d\n"
      ],
      "metadata": {
        "id": "JvTCtlWJCMnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Autoencoder_CNN()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= 1e-3, weight_decay=1e-5)\n"
      ],
      "metadata": {
        "id": "QpczAn6FsZpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "outputs = []\n",
        "for epoch in range(num_epochs):\n",
        "  for (img,_) in data_loader:\n",
        "    #img = img.reshape(-1,28*28)\n",
        "    recon = model(img)\n",
        "    loss = criterion(recon,img)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  print(f\"Epoch:{epoch+1}, Loss:{loss.item(): .4f}\")\n",
        "  outputs.append((epoch,img,recon))\n"
      ],
      "metadata": {
        "id": "JZjQBQFzyXfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in range(0,num_epochs,4):\n",
        "  plt.figure(figsize=(9,2))\n",
        "  plt.gray()\n",
        "  imgs = outputs[k][1].detach().numpy()\n",
        "  recon = outputs[k][2].detach().numpy()\n",
        "  for i, item in enumerate(imgs):\n",
        "    if i>= 9: break   # Plot the first 9 images\n",
        "    plt.subplot(2,9,i+1)\n",
        "    #item = item.reshape(-1,28,28)\n",
        "    # item:1,28,28\n",
        "    plt.imshow(item[0])\n",
        "\n",
        "  for i, item in enumerate(recon):\n",
        "    if i>= 9: break  # Plot the first 9 reconstructed images\n",
        "    plt.subplot(2,9,9+i+1)\n",
        "    #item = item.reshape(-1,28,28)\n",
        "    # item:1,28,28\n",
        "    plt.imshow(item[0])"
      ],
      "metadata": {
        "id": "cfFYTg0J2puk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}